# ============================================================================
# Optuna Hyperparameter Optimization Configuration
# ============================================================================
# Defines HPO study settings, pruning strategy, and search space
# Run via: python -m Project.cli command=hpo n_trials=100
# ============================================================================

# ============================================================================
# OPTUNA STUDY SETTINGS
# ============================================================================

# n_trials: Number of hyperparameter combinations to evaluate
#   - Default: 100 (paired with 100-epoch folds and patience 20)
#   - Each trial runs full 100-epoch K-fold CV (patience 20)
#   - Higher = better hyperparameters but longer runtime
#   - Typical values: 50-200 for medium search spaces; >200 for exhaustive search
n_trials: 100

# storage: Database backend for storing trial history
#   - PostgreSQL recommended for parallel HPO (better concurrency)
#   - Format: postgresql://user:password@host:port/database
#   - Override via CLI: hpo.storage=postgresql://user:pass@host:5432/optuna
#   - Or use environment variables: ${oc.env:POSTGRES_URL}
#   - Alternatives:
#     * sqlite:///optuna.db (local SQLite file, limited concurrency) [default]
#     * mysql://user:pass@host/db (production alternative)
#   - Allows resuming interrupted HPO studies
#   - Enables parallel optimization across multiple processes
#   - Default: SQLite for ease of use; set OPTUNA_STORAGE to Postgres when available
storage: ${oc.env:OPTUNA_STORAGE,sqlite:///optuna.db}

# study_name: Unique identifier for this optimization study
#   - Used to group related trials together
#   - Can resume study with same name: load_if_exists=True
#   - Best practice: descriptive name like "dsm5_transformer_baseline_v1"
study_name: transformer_hpo

# direction: Optimization direction for objective metric
#   - maximize: For metrics like F1, accuracy, AUC (default)
#   - minimize: For loss, error rate
#   - This study maximizes mean F1 across K-fold validation
direction: maximize

# ============================================================================
# PRUNING CONFIGURATION
# ============================================================================
pruner:
  # Hyperband aggressively prunes under-performing trials (critical for large trial counts)
  type: HyperbandPruner
  min_resource: 1
  max_resource: 100
  reduction_factor: 3

# ============================================================================
# HYPERPARAMETER SEARCH SPACE
# ============================================================================
# Defines ranges for each hyperparameter to optimize
# Note: type field is for documentation only (actual sampling in cli.py)

search_space:
  # learning_rate: AdamW learning rate
  #   - type: loguniform - samples from log scale (better for orders of magnitude)
#   - Range: [5e-6, 3e-5] - conservative for base encoders
#   - Examples: 6e-6, 1e-5, 2e-5
#   - Log scale ensures good coverage of 5e-6, 1e-5, 3e-5
  learning_rate:
    type: loguniform
    low: 5e-6
    high: 3e-5

  # batch_size: Training batch size (effective batch simulated via grad accumulation)
  batch_size:
    type: categorical
    choices: [16, 32]

  # weight_decay: L2 regularization strength for AdamW
  #   - type: loguniform - log scale for order of magnitude search
  #   - Range: [0.001, 0.1] - wider than default 0.01
  #   - Examples: 0.001, 0.01, 0.05, 0.1
  #   - Prevents overfitting by penalizing large weights
  weight_decay:
    type: loguniform
    low: 0.001
    high: 0.1

  # warmup_ratio: Fraction of steps for LR warmup
  #   - type: uniform - linear sampling from range
  #   - Range: [0.05, 0.15] - 5% to 15% of total steps
  #   - Lower = shorter warmup (faster convergence, may be unstable)
  #   - Higher = longer warmup (more stable, slower convergence)
  #   - Warmup prevents initial instability in Transformer fine-tuning
  warmup_ratio:
    type: uniform
    low: 0.0
    high: 0.2

  # classifier_head architecture search
  classifier_head:
    type: categorical
    choices: ["linear", "mean_pooling", "max_pooling", "attention_pooling", "mlp1"]

  # classifier_dropout: Dropout on classification head
  classifier_dropout:
    type: uniform
    low: 0.1
    high: 0.5

  # pos_weight: Positive class weight for imbalance handling (Weighted BCE)
  pos_weight:
    type: uniform
    low: 1.0
    high: 5.0

  # use_augmentation: Toggle evidence-span augmentation
  aug_enable:
    type: categorical
    choices: [true, false]

  aug_prob:
    type: uniform
    low: 0.1
    high: 0.7

  aug_method:
    type: categorical
    choices: ["nlpaug_synonym", "nlpaug_contextual"]

  imbalance_mode:
    type: categorical
    choices: ["minority_only", "all"]
